{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71xjpVtVJhU9"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class StorageImageDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Custom data generator to fetch batches from Cloud Storage.\n",
    "\n",
    "    This generator is really inefficient, calls to Cloud Storage should be asynchronous\n",
    "    and could benefit from multithreading. For example purpose only.\n",
    "\n",
    "    For a given project, you can have different versions of a same dataset,\n",
    "    e.g. full res images and downscaled images, raw images and processed images, etc.\n",
    "    It is generally a good idea to treat these versions as different datasets.\n",
    "    \"\"\"\n",
    "    def __init__(self, bucket_name, dataset=\"half_res\", fold=\"train\", batch_size=32):\n",
    "        self.bucket_name = bucket_name\n",
    "        self.dataset = dataset\n",
    "        self.fold = fold # train, validation, test\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Anonymous client is required to query data from public buckets\n",
    "        self.client = storage.Client.create_anonymous_client()\n",
    "\n",
    "        self.df = self.make_df() # Simple dataframe to map input images and output masks\n",
    "        self.indices = self.df.index.to_list() # Dataframe indices\n",
    "\n",
    "        # Callback function\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def read_img(self, img_path):\n",
    "        \"\"\"Utility function to load images from Cloud Storage.\"\"\"\n",
    "        blob = self.client.bucket(self.bucket_name).blob(img_path)\n",
    "        return Image.open(BytesIO(blob.download_as_bytes()))\n",
    "\n",
    "    def make_df(self):\n",
    "        \"\"\"Map each image filepath with the corresponding mask filepath.\"\"\"\n",
    "\n",
    "        # It assumes images and masks have the same name.\n",
    "        images = list(\n",
    "            self.client.list_blobs(\n",
    "              bucket_or_name=self.bucket_name,\n",
    "              prefix=f\"{self.dataset}/{self.fold}/images\"\n",
    "            )\n",
    "        )\n",
    "        return pd.DataFrame({\n",
    "            \"X\": [img.name for img in images],\n",
    "            \"y\": [img.name.replace(\"images\", \"masks\") for img in images]\n",
    "        })\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches generated per epoch.\"\"\"\n",
    "        return len(self.df) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return the i-th batch as a tuple (batch_X, batch_y).\n",
    "\n",
    "        This is where you define the batch generation logic. This can be as custom\n",
    "        as you want. You can scale and transform your data here if needed, load your\n",
    "        data from anywhere, into any shape.\n",
    "        \"\"\"\n",
    "        batch = self.indices[index * self.batch_size:self.batch_size * (1 + index)]\n",
    "        X = self.df.iloc[batch][\"X\"]\n",
    "        y = self.df.iloc[batch][\"y\"]\n",
    "        return self.__get_X(X), self.__get_y(y)\n",
    "\n",
    "    def __get_X(self, X):\n",
    "        \"\"\"Read the batch of image filepaths into a numpy array. (batch_size, X.width, X.height, nb_channels)\"\"\"\n",
    "        img_list = []\n",
    "        for img_path in X:\n",
    "            img = self.read_img(img_path)\n",
    "            img = np.asarray(img) / 255.0\n",
    "            img_list.append(img)\n",
    "        return np.array(img_list)\n",
    "\n",
    "    def __get_y(self, y):\n",
    "      \"\"\"Read the batch of mask filepaths into a numpy array. (batch_size, X.width, X.height, nb_channels)\"\"\"\n",
    "        img_list = []\n",
    "        for img_path in y:\n",
    "            img = self.read_img(img_path)\n",
    "            img = np.asarray(img) / 255.0\n",
    "            img_list.append(img)\n",
    "        return np.array(img_list)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "      \"\"\"Generally used to shuffle indices at the end of a training epoch.\"\"\"\n",
    "      pass\n",
    "\n",
    "train_gen = StorageImageDataGenerator(bucket_name=\"breast-ultrasound-images\", fold=\"train\")\n",
    "test_gen = StorageImageDataGenerator(bucket_name=\"breast-ultrasound-images\", fold=\"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn-5Qov1eAzR"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dibM5RaHeCVN",
    "outputId": "35fe22cd-8bc2-4cc8-faaa-4f8595c24f92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 128, 128, 64)         1792      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 64)         36928     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 64)           0         ['conv2d_1[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 64, 64, 128)          73856     ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 128)          147584    ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 128)          0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 256)          295168    ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 32, 32, 256)          590080    ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 256)          0         ['conv2d_5[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 512)          1180160   ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 16, 16, 512)          2359808   ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2  (None, 32, 32, 512)          0         ['conv2d_7[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 32, 32, 768)          0         ['conv2d_5[0][0]',            \n",
      "                                                                     'up_sampling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 32, 32, 256)          1769728   ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 32, 32, 256)          590080    ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSamplin  (None, 64, 64, 256)          0         ['conv2d_9[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 64, 64, 384)          0         ['conv2d_3[0][0]',            \n",
      " )                                                                   'up_sampling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 64, 64, 128)          442496    ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 64, 64, 128)          147584    ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSamplin  (None, 128, 128, 128)        0         ['conv2d_11[0][0]']           \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 128, 128, 192)        0         ['conv2d_1[0][0]',            \n",
      " )                                                                   'up_sampling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 128, 128, 64)         110656    ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 128, 128, 64)         36928     ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 128, 128, 1)          65        ['conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7782913 (29.69 MB)\n",
      "Trainable params: 7782913 (29.69 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def unet_model(input_size=(128, 128, 3)):\n",
    "    inputs = tf.keras.Input(shape=input_size)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    # Bottom\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
    "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "\n",
    "    # Decoder\n",
    "    up5 = layers.UpSampling2D(size=(2, 2))(conv4)\n",
    "    concat5 = layers.concatenate([conv3, up5], axis=-1)\n",
    "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(concat5)\n",
    "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = layers.UpSampling2D(size=(2, 2))(conv5)\n",
    "    concat6 = layers.concatenate([conv2, up6], axis=-1)\n",
    "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(concat6)\n",
    "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = layers.UpSampling2D(size=(2, 2))(conv6)\n",
    "    concat7 = layers.concatenate([conv1, up7], axis=-1)\n",
    "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(concat7)\n",
    "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
    "\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv7)  # Adjust the number of output channels based on your task\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "# Create the U-Net model\n",
    "model = unet_model(input_size=(128, 128, 3))\n",
    "model.compile(loss = 'mean_squared_error', optimizer = Adam(learning_rate = 0.00005))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4FfjXz_Ae4a7",
    "outputId": "0127c88b-393e-48fb-9b26-85a26656d32a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "12/12 [==============================] - 634s 52s/step - loss: 0.2316 - val_loss: 0.1686\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 595s 49s/step - loss: 0.0569 - val_loss: 0.0017\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 595s 49s/step - loss: 4.2902e-04 - val_loss: 4.3873e-05\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 598s 49s/step - loss: 2.1905e-05 - val_loss: 1.2980e-05\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 605s 50s/step - loss: 9.4926e-06 - val_loss: 8.7729e-06\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    validation_data=test_gen,\n",
    "    epochs = 1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
